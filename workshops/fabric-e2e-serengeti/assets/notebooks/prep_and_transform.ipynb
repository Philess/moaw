{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the annotations in the train table from the lakehouse\n",
    "df = spark.sql(\"SELECT * FROM SnapshotSerengeti_LH.train_annotations WHERE train_annotations.category_id > 1\")\n",
    "\n",
    "# filter out the season, sequence ID, category_id snf image_id\n",
    "df_train = df.select(\"season\", \"seq_id\", \"category_id\", \"location\", \"image_id\", \"datetime\")\n",
    "\n",
    "# remove image_id wiTH null and duplicates\n",
    "df_train = df_train.filter(df_train.image_id.isNotNull()).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from pyspark.sql.functions import split, regexp_replace, col\n",
    "\n",
    "# This splits the seq_id string at '#' and takes the first part.\n",
    "df_train = df_train.withColumn(\"season_extracted\", split(col(\"seq_id\"), \"#\").getItem(0))\n",
    "\n",
    "# Remove the 'SER_' prefix from the extracted season for better readability.\n",
    "df_train = df_train.withColumn(\"season_label\", regexp_replace(col(\"season_extracted\"), \"SER_\", \"\"))\n",
    "\n",
    "# Group by the season_label and count the number of sequences for each season, then order the results.\n",
    "df_counts = df_train.groupBy(\"season_label\").count().orderBy(\"season_label\")\n",
    "\n",
    "# visualize the spark data frame directly in the notebook\n",
    "display(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fce269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Compute the number of images per sequence.\n",
    "seq_counts = df_train.groupBy(\"seq_id\").count()\n",
    "\n",
    "# Aggregate the data: group by the image count and count how many sequences have that count.\n",
    "sequence_length_counts = (seq_counts\n",
    "    .groupBy(\"count\")\n",
    "    .agg(F.count(\"seq_id\").alias(\"Count of sequences\"))\n",
    "    .withColumnRenamed(\"count\", \"Number of images\")\n",
    "    .orderBy(F.col(\"Number of images\"))\n",
    ")\n",
    "\n",
    "# visualize the spark data frame directly in the notebook\n",
    "display(sequence_length_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d985fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "def transform_image_data(df, categories_df):\n",
    "    \"\"\"\n",
    "    Joins the input DataFrame with the categories DataFrame, renames columns,\n",
    "    and appends a '.JPG' extension to the filename column.\n",
    "    \"\"\"\n",
    "    # Join on category_id to map category names. The join brings the category name as \"name\", which we then rename to \"label\".\n",
    "    df = df.join(\n",
    "        categories_df.select(col(\"id\").alias(\"category_id\"), col(\"name\")),\n",
    "        on=\"category_id\",\n",
    "        how=\"left\"\n",
    "    ).withColumnRenamed(\"name\", \"label\")\n",
    "\n",
    "    # Drop the 'category_id' column\n",
    "    df = df.drop(\"category_id\")\n",
    "\n",
    "    # Rename 'image_id' to 'filename'\n",
    "    df = df.withColumnRenamed(\"image_id\", \"filename\")\n",
    "\n",
    "    # Append '.JPG' to the filename\n",
    "    df = df.withColumn(\"filename\", concat(col(\"filename\"), lit(\".JPG\")))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the categories table as a Spark DataFrame\n",
    "categories_df = spark.sql(\"SELECT * FROM SnapshotSerengeti_LH.categories\")\n",
    "\n",
    "# Apply transformation for image data\n",
    "df_train = transform_image_data(df_train, categories_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696afdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define a window partitioned by seq_id and ordered by filename\n",
    "windowSpec = Window.partitionBy(\"seq_id\").orderBy(\"filename\")\n",
    "\n",
    "# Assign a row number to each row within its sequence, filter for the first frame, then drop the helper column.\n",
    "df_train = df_train.withColumn(\"row_num\", row_number().over(windowSpec)) \\\n",
    "                   .filter(col(\"row_num\") == 1) \\\n",
    "                   .drop(\"row_num\")\n",
    "\n",
    "# Count the rows in the resulting DataFrame\n",
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame that counts the number of images per label\n",
    "label_counts = df_train.groupBy(\"label\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Rename the columns for better readability and visualization\n",
    "label_counts = label_counts.withColumnRenamed(\"label\", \"Label\") \\\n",
    "                           .withColumnRenamed(\"count\", \"Number of images\")\n",
    "\n",
    "# Visualize the label counts\n",
    "display(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04985396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ImageUrl(filename):\n",
    "    return f\"https://lilawildlife.blob.core.windows.net/lila-wildlife/snapshotserengeti-unzipped/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286258ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a UDF from the function\n",
    "get_ImageUrl_udf = udf(get_ImageUrl, StringType())\n",
    "\n",
    "# Apply the UDF to create the image_url column\n",
    "df_train = df_train.withColumn(\"image_url\", get_ImageUrl_udf(col(\"filename\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23567837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_random_image(label, random_state, width=500):\n",
    "    # Filter the Spark DataFrame to only include rows with the specified label,\n",
    "    # then order randomly (using the provided seed) and select one row.\n",
    "    row = df_train.filter(col(\"label\") == label) \\\n",
    "                  .orderBy(F.rand(random_state)) \\\n",
    "                  .limit(1) \\\n",
    "                  .collect()[0]\n",
    "    \n",
    "    # Get the image URL from the selected row and display the image\n",
    "    url = row[\"image_url\"]\n",
    "    download_and_display_image(url, label)\n",
    "\n",
    "def download_and_display_image(url, label):\n",
    "    image = plt.imread(urllib.request.urlopen(url), format='jpg')\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_image(label='leopard', random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportional_allocation_percentage(df, percentage):\n",
    "    \"\"\"\n",
    "    Proportionally allocate a sample of 'percentage'% of df across \n",
    "    groups (label, season, location)\n",
    "    \"\"\"\n",
    "    # Determine the total number of rows and desired sample size\n",
    "    total_count = df.count()\n",
    "    sample_size = int(round(total_count * (percentage / 100.0)))\n",
    "\n",
    "    # Compute group counts\n",
    "    group_counts = (\n",
    "        df.groupBy(\"label\", \"season\", \"location\")\n",
    "          .count()  # number of rows in each group\n",
    "          .withColumnRenamed(\"count\", \"group_count\")\n",
    "    )\n",
    "\n",
    "    # Compute the proportion of each group, then approximate \"sample_needed\" via rounding\n",
    "    group_counts = (\n",
    "        group_counts\n",
    "        .withColumn(\"proportion\", F.col(\"group_count\") / F.lit(total_count))\n",
    "        .withColumn(\"sample_needed\", F.round(F.col(\"proportion\") * sample_size).cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    #  Collect just the group-level info to the driver for fine-grained adjustment\n",
    "    group_counts_pd = group_counts.select(\n",
    "        \"label\", \"season\", \"location\", \"group_count\", \"sample_needed\", \"proportion\"\n",
    "    ).toPandas()\n",
    "\n",
    "    # Sum of \"sample_needed\" might not equal the total desired sample_size due to rounding\n",
    "    current_sum = group_counts_pd[\"sample_needed\"].sum()\n",
    "    difference = sample_size - current_sum\n",
    "\n",
    "    if difference > 0:\n",
    "        # If we're short, we add +1 to the groups with the largest proportions until we fix the difference\n",
    "        # Sort descending by proportion\n",
    "        group_counts_pd = group_counts_pd.sort_values(\"proportion\", ascending=False)\n",
    "        for i in range(difference):\n",
    "            group_counts_pd.iat[i, group_counts_pd.columns.get_loc(\"sample_needed\")] += 1\n",
    "        # Re-sort back if desired\n",
    "        group_counts_pd = group_counts_pd.sort_values([\"label\", \"season\", \"location\"])\n",
    "    elif difference < 0:\n",
    "        # If we have too many, subtract 1 from the groups with the smallest proportions\n",
    "        # Sort ascending by proportion\n",
    "        group_counts_pd = group_counts_pd.sort_values(\"proportion\", ascending=True)\n",
    "        for i in range(abs(difference)):\n",
    "            group_counts_pd.iat[i, group_counts_pd.columns.get_loc(\"sample_needed\")] -= 1\n",
    "        # Re-sort back if desired\n",
    "        group_counts_pd = group_counts_pd.sort_values([\"label\", \"season\", \"location\"])\n",
    "\n",
    "    # Create a Spark DataFrame of the final sample allocations\n",
    "    allocations_sdf = spark.createDataFrame(group_counts_pd)\n",
    "\n",
    "    #  Join the allocations back to the main DataFrame so each row knows how many rows \n",
    "    #    from its group we want to keep\n",
    "    df_joined = (\n",
    "        df.join(\n",
    "            F.broadcast(allocations_sdf),\n",
    "            on=[\"label\", \"season\", \"location\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Use a row_number partitioned by (label, season, location) to limit how many rows per group\n",
    "    window_spec = Window.partitionBy(\"label\", \"season\", \"location\").orderBy(F.monotonically_increasing_id())\n",
    "    df_with_rn = df_joined.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "\n",
    "    # Filter out rows where 'rn' exceeds 'sample_needed'\n",
    "    df_sample = df_with_rn.filter(F.col(\"rn\") <= F.col(\"sample_needed\"))\n",
    "\n",
    "    # Drop helper columns if you don't need them in the final result\n",
    "    df_sample = df_sample.drop(\"proportion\", \"group_count\", \"sample_needed\", \"rn\")\n",
    "\n",
    "    return df_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eacc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.05\n",
    "sampled_train = proportional_allocation_percentage(df_train, percent)\n",
    "\n",
    "# Group by the season_label and count the number of sequences for each season, then order the results.\n",
    "df_sampled_train_counts = sampled_train.groupBy(\"season_label\").count().orderBy(\"season_label\")\n",
    "\n",
    "# visualize the spark data frame directly in the notebook\n",
    "display(df_sampled_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def download_and_resize_image(url, path, kind):\n",
    "    filename = os.path.basename(path)\n",
    "    directory = os.path.dirname(path)\n",
    "\n",
    "    # Define a new directory path where permission is granted\n",
    "    directory_path = f'/lakehouse/default/Files/images/{kind}/{directory}/'\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    # Define the full target file path\n",
    "    target_file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "    # Check if file already exists\n",
    "    if os.path.isfile(target_file_path):\n",
    "        return\n",
    "\n",
    "    # Download the image\n",
    "    urllib.request.urlretrieve(url, target_file_path)\n",
    "\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(target_file_path)\n",
    "\n",
    "    # Resize the image to a reasonable ML training size\n",
    "    resized_img = img.resize((224, 224), Image.ANTIALIAS)\n",
    "\n",
    "    # Save the resized image to a defined filepath\n",
    "    resized_img.save(target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74042439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def execute_parallel_download(spark_df, kind):\n",
    "    df = spark_df.toPandas()\n",
    "    # Use a process pool instead of a thread pool to avoid thread safety issues\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Batch process images instead of processing them one at a time\n",
    "        urls = df['image_url'].tolist()\n",
    "        paths = df['filename'].tolist()\n",
    "        futures = [executor.submit(download_and_resize_image, url, path, kind) for url, path in zip(urls, paths)]\n",
    "        # Wait for all tasks to complete\n",
    "        concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f70292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.sql(\"SELECT * FROM SnapshotSerengeti_LH.test_annotations WHERE test_annotations.category_id > 1\")\n",
    "\n",
    "\n",
    "df_test = (\n",
    "    df_test\n",
    "    .filter(df_test.image_id.isNotNull())\n",
    "    .dropDuplicates()\n",
    "    .withColumn(\"season_extracted\", split(col(\"seq_id\"), \"#\").getItem(0))\n",
    "    .withColumn(\"season_label\", regexp_replace(col(\"season_extracted\"), \"SER_\", \"\")))\n",
    "\n",
    "df_test = transform_image_data(df_test, categories_df)\n",
    "df_test = df_test.withColumn(\"image_url\", get_ImageUrl_udf(col(\"filename\")))\n",
    "\n",
    "sampled_test = proportional_allocation_percentage(df_test, 0.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80489bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "execute_parallel_download(sampled_train, 'train')\n",
    "execute_parallel_download(sampled_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f53803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_all_files(directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "train_images_path = f\"/lakehouse/default/Files/images/train/\"\n",
    "test_images_path =  f\"/lakehouse/default/Files/images/test/\"\n",
    "\n",
    "print(f\"{len(list_all_files(train_images_path))} files downloaded out of {sampled_train.count()}\")\n",
    "print(f\"{len(list_all_files(test_images_path))} files downloaded out of {sampled_test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ed43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train.select(\"filename\", \"label\")\\\n",
    "    .write.saveAsTable(\"sampled_train\", mode=\"overwrite\", overwriteSchema=\"true\")\n",
    "\n",
    "sampled_test.select(\"filename\", \"label\")\\\n",
    "    .write.saveAsTable(\"sampled_test\", mode=\"overwrite\", overwriteSchema=\"true\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "notebook_environment": {},
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "5f58e739-d87b-404a-9a7b-5e64738a82c5",
    "default_lakehouse_name": "Serengeti_Lakehouse",
    "default_lakehouse_workspace_id": "1eaa972c-c94a-47fa-90f5-bcad0a19f945",
    "known_lakehouses": [
     {
      "id": "5f58e739-d87b-404a-9a7b-5e64738a82c5"
     }
    ]
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
